<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Quicktour ‚Ä¢ hftokenizers</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Quicktour">
<meta property="og:description" content="hftokenizers">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">hftokenizers</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/quicktour.html">Quicktour</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="quicktour_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Quicktour</h1>
            
      
      
      <div class="hidden name"><code>quicktour.Rmd</code></div>

    </div>

    
    
<blockquote>
<p>Port of the Quicktour vignette in ü§ó tokenizers website. Click <a href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch">here</a> for the original version.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlverse.github.io/hftokenizers">hftokenizers</a></span><span class="op">)</span></code></pre></div>
<p>Let‚Äôs have a quick look at the ü§ó Tokenizers library features. The library provides an implementation of today‚Äôs most used tokenizers that is both easy to use and blazing fast.</p>
<p>It can be used to instantiate a pretrained tokenizer but we will start our quicktour by building one from scratch and see how we can train it.</p>
<div id="build-a-tokenizer-from-scratch" class="section level2">
<h2 class="hasAnchor">
<a href="#build-a-tokenizer-from-scratch" class="anchor"></a>Build a tokenizer from scratch</h2>
<p>To illustrate how fast the ü§ó Tokenizers library is, let‚Äôs train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. First things first, you will need to download this dataset and unzip it with:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">url</span> <span class="op">&lt;-</span> <span class="st">"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"</span>
<span class="va">fpath</span> <span class="op">&lt;-</span> <span class="fu">pins</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/pins/man/pin.html">pin</a></span><span class="op">(</span><span class="va">url</span><span class="op">)</span></code></pre></div>
<div id="training-the-tokenizer" class="section level3">
<h3 class="hasAnchor">
<a href="#training-the-tokenizer" class="anchor"></a>Training the tokenizer</h3>
<p>In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. For more information about the different type of tokenizers, check out this guide in the ü§ó Transformers documentation. Here, training the tokenizer means it will learn merge rules by:</p>
<ul>
<li>Start with all the characters present in the training corpus as tokens.</li>
<li>Identify the most common pair of tokens and merge it into one token.</li>
<li>Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.</li>
</ul>
<p>The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/tokenizer.html">tokenizer</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>model <span class="op">=</span> <span class="va">models_bpe</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>unk_token <span class="op">=</span> <span class="st">"[UNK]"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>To train our tokenizer on the wikitext files, we will need to instantiate a trainer, in this case a <code>BpeTrainer</code>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/trainers_bpe.html">trainers_bpe</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  special_tokens<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"[UNK]"</span>, <span class="st">"[CLS]"</span>, <span class="st">"[SEP]"</span>, <span class="st">"[PAD]"</span>, <span class="st">"[MASK]"</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We can set the training arguments like <code>vocab_size</code> or <code>min_frequency</code> (here left at their default values of 30,000 and 0) but the most important part is to give the <code>special_tokens</code> we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.</p>
<blockquote>
<p>The order in which you write the special tokens list matters: here ‚Äú[UNK]‚Äù will get the ID 0, ‚Äú[CLS]‚Äù will get the ID 1 and so forth.</p>
</blockquote>
<p>We could train our tokenizer right now, but it wouldn‚Äôt be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an <code>"it is"</code> token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="va">pre_tokenizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pre_tokenizers_whitespace.html">pre_tokenizers_whitespace</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>Now, we can just call the <code>train()</code> method with any list of files we want to use:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">fpath</span>, <span class="va">trainer</span><span class="op">)</span></code></pre></div>
<p>This should only take a few seconds to train our tokenizer on the full wikitext dataset! To save the tokenizer in one file that contains all its configuration and vocabulary, just use the <code><a href="https://rdrr.io/r/base/save.html">save()</a></code> method:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="fu">save</span><span class="op">(</span><span class="st">"tokenizer-wiki.json"</span><span class="op">)</span></code></pre></div>
<p>and you can reload your tokenizer from that file with the <code>from_file()</code> class method:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span> <span class="op">&lt;-</span> <span class="va">tokenizer</span><span class="op">$</span><span class="fu">from_file</span><span class="op">(</span><span class="st">"tokenizer-wiki.json"</span><span class="op">)</span></code></pre></div>
</div>
<div id="using-the-tokenizer" class="section level3">
<h3 class="hasAnchor">
<a href="#using-the-tokenizer" class="anchor"></a>Using the tokenizer</h3>
<p>Now that we have trained a tokenizer, we can use it on any text we want with the <code>encode()</code> method:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">tok</span><span class="op">$</span><span class="fu">encode</span><span class="op">(</span><span class="st">"Hello, y'all! How are you üòÅ ?"</span><span class="op">)</span></code></pre></div>
<p>This applied the full pipeline of the tokenizer on the text, returning an <code>Encoding</code> object. To learn more about this pipeline, and how to apply (or customize) parts of it, check out <a href="TODO">this page</a>.</p>
<p>This <code>Encoding</code> object then has all the attributes you need for your deep learning model (or other). The <code>tokens</code> attribute contains the segmentation of your text in tokens:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">$</span><span class="va">tokens</span>
<span class="co">#&gt;  [1] "Hello" ","     "y"     "'"     "all"   "!"     "How"   "are"   "you"  </span>
<span class="co">#&gt; [10] "[UNK]" "?"</span></code></pre></div>
<p>Similarly, the <code>ids</code> attribute will contain the index of each of those tokens in the tokenizer‚Äôs vocabulary:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">$</span><span class="va">ids</span>
<span class="co">#&gt;  [1] 27253    16    93    11  5097     5  7961  5112  6218     0    35</span></code></pre></div>
<p>An important feature of the ü§ó Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our <code>Encoding</code> object. For instance, let‚Äôs assume we would want to find back what caused the <code>"[UNK]"</code> token to appear, which is the token at index 10 in the list, we can just ask for the offset at the index:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">$</span><span class="va">offsets</span><span class="op">[[</span><span class="fl">10</span><span class="op">]</span><span class="op">]</span>
<span class="co">#&gt; [1] 26 27</span></code></pre></div>
<p>and those are the indices that correspond to the emoji in the original sentence:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/substr.html">substr</a></span><span class="op">(</span><span class="st">"Hello, y'all! How are you üòÅ ?"</span>, <span class="fl">26</span>, <span class="fl">27</span><span class="op">)</span>
<span class="co">#&gt; [1] " üòÅ"</span></code></pre></div>
</div>
<div id="post-processing" class="section level3">
<h3 class="hasAnchor">
<a href="#post-processing" class="anchor"></a>Post-processing</h3>
<p>We might want our tokenizer to automatically add special tokens, like <code>"[CLS]"</code> or <code>"[SEP]"</code>. To do this, we use a post-processor. <code>TemplateProcessing</code> is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.</p>
<p>When we built our tokenizer, we set <code>"[CLS]"</code> and <code>"[SEP]"</code> in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the <code>token_to_id()</code> method:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="fu">token_to_id</span><span class="op">(</span><span class="st">"[SEP]"</span><span class="op">)</span>
<span class="co">#&gt; [1] 2</span></code></pre></div>
<p>Here is how we can set the post-processing to give us the traditional BERT inputs:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="va">post_processor</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/post_processors_template_processing.html">post_processors_template_processing</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  single<span class="op">=</span><span class="st">"[CLS] $A [SEP]"</span>,
  pair<span class="op">=</span><span class="st">"[CLS] $A [SEP] $B:1 [SEP]:1"</span>,
  special_tokens<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
     <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"[CLS]"</span>, <span class="va">tok</span><span class="op">$</span><span class="fu">token_to_id</span><span class="op">(</span><span class="st">"[CLS]"</span><span class="op">)</span><span class="op">)</span>,
     <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"[SEP]"</span>, <span class="va">tok</span><span class="op">$</span><span class="fu">token_to_id</span><span class="op">(</span><span class="st">"[SEP]"</span><span class="op">)</span><span class="op">)</span>
  <span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>Let‚Äôs go over this snippet of code in more details. First we specify the template for single sentences: those should have the form <code>"[CLS] $A [SEP]"</code> where <code>$A</code> represents our sentence.</p>
<p>Then, we specify the template for sentence pairs, which should have the form <code>"[CLS] $A [SEP] $B [SEP]"</code> where <code>$A</code> represents the first sentence and <code>$B</code> the second one. The <code>:1</code> added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don‚Äôt have <code>$A:0</code>) and here we set it to 1 for the tokens of the second sentence and the last <code>"[SEP]"</code> token.</p>
<p>Lastly, we specify the special tokens we used and their IDs in our tokenizer‚Äôs vocabulary.</p>
<p>To check out this worked properly, let‚Äôs try to encode the same sentence as before:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="fu">encode</span><span class="op">(</span><span class="st">"Hello, y'all! How are you üòÅ ?"</span><span class="op">)</span><span class="op">$</span><span class="va">tokens</span>
<span class="co">#&gt;  [1] "[CLS]" "Hello" ","     "y"     "'"     "all"   "!"     "How"   "are"  </span>
<span class="co">#&gt; [10] "you"   "[UNK]" "?"     "[SEP]"</span></code></pre></div>
<p>To check the results on a pair of sentences, we just pass the two sentences to <code>encode()</code>:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">tok</span><span class="op">$</span><span class="fu">encode</span><span class="op">(</span><span class="st">"Hello, y'all!"</span>, <span class="st">"How are you üòÅ ?"</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">$</span><span class="va">tokens</span>
<span class="co">#&gt;  [1] "[CLS]" "Hello" ","     "y"     "'"     "all"   "!"     "[SEP]" "How"  </span>
<span class="co">#&gt; [10] "are"   "you"   "[UNK]" "?"     "[SEP]"</span></code></pre></div>
<p>You can then check the type IDs attributed to each token is correct with</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">$</span><span class="va">type_ids</span>
<span class="co">#&gt;  [1] 0 0 0 0 0 0 0 0 1 1 1 1 1 1</span></code></pre></div>
<p>If you save your tokenizer with <code><a href="https://rdrr.io/r/base/save.html">save()</a></code>, the post-processor will be saved along.</p>
</div>
</div>
<div id="encoding-multiple-sentences-in-a-batch" class="section level2">
<h2 class="hasAnchor">
<a href="#encoding-multiple-sentences-in-a-batch" class="anchor"></a>Encoding multiple sentences in a batch</h2>
<p>To get the full speed of the ü§ó Tokenizers library, it‚Äôs best to process your texts by batches by using the <code>encode_batch()</code> method:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">tok</span><span class="op">$</span><span class="fu">encode_batch</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Hello, y'all!"</span>, <span class="st">"How are you üòÅ ?"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>The output is then a list of <code>Encoding</code> objects like the ones we saw before. You can process together as many texts as you like, as long as it fits in memory.</p>
<p>To process a batch of sentences pairs, pass two lists to the <code>encode_batch()</code> method: the list of sentences A and the list of sentences B:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">tok</span><span class="op">$</span><span class="fu">encode_batch</span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Hello, y'all!"</span>, <span class="st">"How are you üòÅ ?"</span><span class="op">)</span>, 
    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Hello to you too!"</span>, <span class="st">"I'm fine, thank you!"</span><span class="op">)</span>
  <span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using <code>enable_padding()</code>, with the <code>pad_token</code> and its ID (which we can double-check the id for the padding token with <code>token_to_id()</code> like before):</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tok</span><span class="op">$</span><span class="fu">enable_padding</span><span class="op">(</span>pad_id<span class="op">=</span><span class="fl">3</span>, pad_token<span class="op">=</span><span class="st">"[PAD]"</span><span class="op">)</span></code></pre></div>
<p>We can set the direction of the padding (defaults to the right) or a given length if we want to pad every sample to that specific number (here we leave it unset to pad to the size of the longest text).</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">tok</span><span class="op">$</span><span class="fu">encode_batch</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Hello, y'all!"</span>, <span class="st">"How are you üòÅ ?"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">tokens</span>
<span class="co">#&gt; [1] "[CLS]" "How"   "are"   "you"   "[UNK]" "?"     "[SEP]" "[PAD]"</span></code></pre></div>
<p>In this case, the attention mask generated by the tokenizer takes the padding into account:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">output</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">attention_mask</span>
<span class="co">#&gt; [1] 1 1 1 1 1 1 1 0</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Daniel Falbel.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
